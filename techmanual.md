# Technical Manual

## Introduction to the BCK

This technical manual provides an overview of the development and structure of the BCK. For an introduction to the use of the BCK through its GUI interface, please refer to the User Manual. The scope of this document includes a description of the overall class structure of the BCK.ANN package and a brief overview of the class structure of the BCK.GUI package.


## The BCK.ANN Package

### Introduction

The BCK.ANN is a Java package, which contains a group of classes designed to facilitate an Object Oriented, reusable resource for Neural Network practitioners. It is intended for the use of Java programmers who want to add a Neural Network component to an application, without having to code such functionality from scratch. Additionally, the BCK.ANN package conforms to good coding standards and is easily extended, which ensures that code which uses the package can be easily upgraded and maintained.

The primary component of the package is a class heirarchy of Neural Networks. The base class, BCKNeuralNetwork, provides a very generalised object type, which is used as the base class for all the implemented ANN architectures. Anyone wishing to create a new ANN architecture is advised to extend either the BCKNeuralNetwork class, or one of its subclasses.

The documentation generated by javadoc is available at: http://www.compapp.dcu.ie/~tdoris.ca4/javadoc . This documentation provides most of the information programmers will need when integrating BCK.ANN into their application. The source code obviously provides a more detailed view of the inner workings of the package, but an understanding of the code is not necessary for the successful use of the package.

### Implementation Environment

We developed the BCK under Solaris 2.5 on Dell Pentiums, using the JDK1.1.5. We coded in Xemacs, using the Java-mode syntax highlighting plug-in. Standard make files were used at the early stages of the project for compilation, but as the project became larger, this technique produced too much overhead as compile times became very long. To surmount this, we developed a script, 'tmake' which recognises the implicit dependency between each '.java' source file and its associated '.class' file, and only re-compiled the necessary files.

For the purposes of Software Metrics collection, we developed the 'Describe' script, which reports details of each ',java' file in the current directory. The output includes, class name, LOC, and super class name. Another script, DescribeI, also prints details of the interface methods of the classes in the '.java' files.


Design

The BCK.ANN implements a truly Object-Oriented view of modelling Neural Networks. This philosophy was adopted at the earliest stages of the project, and time and again has proven to be worthwhile and beneficial.

Because of the now massive number of ANN architectures in circulation, it was quite a challenge to design a single base class with the required generality, without being devoid of all functionality. Obviously, there is a compromise between generality and functionality in the implemented system. Having used the BCKNeuralNetwork class as the base class to quite a number of architectures, we feel that it provides a close to optimal basis.

An unusual and useful feature of the structure of Neural Networks in BCK.ANN is that the Neural Network object does not model the connections between neurons. Such connections are contained and maintained by the BCKNeuron objects themselves. At first this design seems to imply that performance would be degraded by such a model. However, with the correct choice of method protection modifiers and compiler optimisations, no significant degradation occurs. The great advantage of such a model is that it simplifies greatly the operation of creating architectures with complex connection patterns, and spreads the functionality evenly between the BCKNeuron and BCKNeuralNetwork classes. An additional and very important consequence of this design is that the BCKNeuralNetwork can accept any type of neuron, and does not need to implement any particular transfer or activation calculation methods.

Details

Overview

In all there are 24 classes in the BCK.ANN package. The javadoc documentation and source code provide the detailed information on these classes. What is presented here is intended as a general overview of the most important classes.

The BCK.ANN was designed using OMT WithClass 3.1. The class hierarchy is available in the javadoc documentation referred to previously.

The Observer Pattern

The Observer pattern is used in the BCK.ANN package, via the BCKObserver and BCKObservable interfaces. The BCKNeuralNetwork class implements the BCKObservable interface. This interface provides the methods required for the Observer software pattern. This is useful for GUI objects that wish to reflect the state of the network. See the BCKNetworkEditor class in the BCK.GUI package for details of an implemented observer of the BCKNeuralNetwork.

The BCKBrain Class


The BCKBrain class provides a container object for BCKNeuralNetwork objects. It provides the functionality required for the creation of systems of ANNs in which the output of one network provides the input of one or more others. As such, the BCK.ANN package provides users with a powerful system for the creation of modularised ANN systems, as these are becoming increasingly popular in serious applications. The BCKBrain is best looked at as a Meta-Neural Network. It has a layer of input neurons and a layer of outputs, so that it itself is a subclass of BCKNeuralNetwork. Additionally it maintains a graph of Neural Networks which can be considered to be the hidden layers of the BCKBrain neural network. The manner in which networks are connected is relatively simple, and is modelled on the normal BCKSynapses used in conventional inter-neuron connections within a normal BCKNeuralNetwork.

Proxy Inputs

The BCKBrain maintains a Vector called proxyInputs. This vector contains a vector for each Network in the Brain. These vectors contain BCKNetConnection objects. Each BCKNetConnection object models a connection from one 'source' neuron in a 'source' network to a 'target' neuron in a 'target' network.

There is an important difference between normal synapse and BCKNetConnections, however. During a forward pass through a network, in which an input vector is fed into the input layer and each non-input neuron evaluates a response to the supplied input, synapses are used to transfer the output of one neuron to the input of another. BCKNetConnections cause the output of the source neuron to provide the output of the target neuron, that is, during a forward pass through cascaded networks, the output vector of the source network is used as the input vector for the target network, not as inputs to the neurons in the target network input layer.

BCKNetConnections are fundamental to the correct operation of the BCKBrain object. When classifying data with a system of networks contained in a BCKBrain, the input layer of the brain is used to provide source neurons which networks can connect to to obtain part or all of the supplied input vector. The output neurons of the brain can be connected to the output neurons of networks in the system and thereby obtain the aggregate output of the entire system.

 

Neurons

There are five types of neuron supported in the BCK.ANN package, and the addition of more types is a simple operation. All of the functionality for maintaining and manipulating connections between neurons is implemented in the BCKNeuron base class. Different neuron types generally only differ on the basis of how they compute their activation, or gross input as supplied by the inputs, (the calcActivation() method) and how they transform that activation into an output (the transfer() method). Therefore, to create a new neuron type, simply extend the BCKNeuron class and override the transfer() and calcActivation() methods to specify the behaviour desired.

The five neuron types presently supplied with the BCK.ANN package are:

BCKSigmoidNeuron - activation calculated as the dot product of the weight vector and the output vector supplied by input neurons, the transfer function is the Fermi squashing function. This transfer function gives values between 0 and 1 for all activations.

BCKLinearNeuron - dot product activation, output = activation.

BCKBipolarNeuron - dot product activation, output is either -1 or +1;

BCKBinaryNeuron - dot product activation, output is either 0 or 1

BCKRadialNeuron - activation calculated as the euclidean distance of the weight vector from the input vector, output is calculated using a Gaussian transfer function. This neuron contains an extra parameter, the standard deviation value, for use in the transfer function.


Neural Networks

The BCK.ANN package supports four basic architectures:

The Multi-Layer Perceptrion (MLP) architecture. This architecture is the most popular, flexible and powerful of all conventional ANNs. It consists of an input layer, an output layer and any number of hidden layers of neurons. By default the BCKMLP class creates an MLP with sigmoid neurons, which is the typical choice. The BCK.ANN supports two training algorithms for MLPs, Backpropogation of Errors and Fahlman's Quickprop, via two subclasses of BCKMLP, BCKBprop and BCKQprop respectively.
The Hopfield architecture models associative memory. It has a single layer of bipolar neurons, which are updated stochastically.
The Kohonen architecture is an unsupervised learning architecture which is based on principles of topographic modelling in the brain. It consists of an input layer of dummy neurons and an output layer of radial neurons with fixed standard deviation.
The Radial Basis Function architecture is implemented in the BCKRBF class. This is a relatively new architecture with a broad range of applications and extremely fast learning rates. Unlike the other architectures, which contain a fixed number of nodes, the RBF architecture is dynamic, in that it adds neurons to its hidden layer as it is trained. It consists of three layers, an input layer of dummy neurons, a hidden layer of Radial neurons and an output layer of linear neurons. The BCKRBFDDA architecture extends the BCKRBF and implements the Dynamic Decay Adjustment learning algorithm for RBF networks.
 

The creation of new classes implementing specific ANN architectures should proceed as follows:

Subclass the BCKNeuralNetwork class. If you are creating an MLP with a new training algorithm, subclass the BCKMLP class, and so on.
In the constructor, add the required number of neurons and connect them according to the stipulations of the architecture.
Override the forwardPass() method if your architecture requires that neurons calculate their outputs in an order that is not 'first node added to network first, then the next added...'.
Implement the training algorithm.
Override the classify() method if necessary.
 

Filters and I/O

As described in the User Manual, the BCK assumes a simple file format for training data files.

Each network by definition has a specified number of input and output nodes. It is therefore assumed that a training file will consist of records which contain an input vector and a desired output vector. The fields of a record are space-delimited numbers, in plain text (i.e. human-readable) format. Therefore for an MLP with 10 inputs and 5 outputs, the training file will consist of records with 15 numbers. The first ten fields in each record constitute the input vector, while the last 5 fields constitute the desired output vector.
 

Implementation

The Java code which implements the BCK.ANN is relatively complex in certain respects, since it deals with graph data structures and complex training algorithms. It is not intended that practitioners should read the code of a training algorithm in an attempt to understand how it works. A good theoretical grounding in the algorithm in question is required before any understanding of the code can be achieved, since it is by nature very mathematical. There are many good books and web sites which provide a much better introduction to ANNs than is possible here. A recommended site is Kevin Gurney's : http://www.shef.ac.uk/psychology/gurney/notes/contents.html .

While the code is mathematically founded and therefore complex in parts, it does not use obscure or obfuscated features of the Java language. In fact, apart from a few points mentioned below, all of the code could be easily translated into C or C++.

We have used java Vectors quite a lot, as they provide a very efficient and convenient dynamic storage facility.

All classes in the BCK.ANN are Serializable. We found the Serialization facility to be extremely useful and it undoubtedly saved considerable time and effort.

The BCK.ANN consists of just over 3,000 lines of code. This is much less than our original estimate, and is a testament to the power and elegance of the Java language. A previous project, written in C++, which had less functionality, took an equivalent LOC measure and much greater effort per unit of functionality.

 

Using the BCK.ANN

Any Java developers who want to add neural network functionality to their application can use the BCK.ANN package to achieve this. The BCK.GUI is the prime example of an application which utilises the BCK.ANN package functionality, and the code therein should be the prime reference point for all developers in search of practical, working examples.

The 'javadoc'-generated documentation for the ANN api provides comprehensive information with regard to the structure and interfaces of classes in the BCK.ANN package. The information therein is sufficient for the successful interfacing of client packages, and was our primary source of information during the development of the BCK.GUI package.

While the BCK.ANN package supports four major neural network architectures, we do not claim that this is sufficient for all ANN applications. In fact, it is our express opinion that practitioners with a very specific application should extend the classes of the ANN package and tailor them to the task at hand. This will result in a more efficient and convenient system.


The BCK.GUI Package

Overview

The BCK.GUI is the Java package that implements the GUI of the BCK. We developed it in order to provide a user-friendly interface to the components of the BCK.ANN, so that they could be used by non-programmers to create ANNs. We used the new JFC Swing system from Sun which provides greater functionality than the Original Java AWT.

The BCK.GUI package is in fact much larger than the BCK.ANN package. Our original estimate for LOC was that the GUI would take approximately 2500 LOC. The final LOC count is just over 8000 lines. The huge difference between our initial estimate and the final result can be understood via the following points:

We have continued to add functionality to the design of the GUI throughout its development.
Java requires quite verbose code to implement GUI components.
Code re-use through inheritance is more difficult when classes implement GUI components.
The final LOC measure does not imply that the GUI required more effort than the BCK.ANN package, since the code in the GUI is of a completely different nature to the ANN code.
In retrospect the effort required for the development of the two packages was approximately the same, and our estimate for required effort was reasonably accurate.
 

More detailed design work and more sophisticated software engineering techniques were used in the BCK.ANN package. The design of the GUI basically amounted to the graphical design of dialog boxes and other GUI components. The simplicity of the design stage of the GUI was more than offset by the steep learning curve we experienced with Swing, which is still being developed by Sun, and exhibits some quite strange behaviours.

While there are a number of 'GUI Builders' available for Java, none of those available to us satisfied our requirements. For this reason, we decided that we would hand-code all of the GUI. This gave us finer control and better knowledge of the inner workings of our system than would have been possible with tool-generated code. We believe in hindsight that the savings in effort through the use of tools at the early stages of GUI development would have been negated by the additional effort required at later stages when we would have had to implement the finer details of event handling, and interfacing with the BCK.ANN package.

Our primary goal during GUI development was that all of the functionality available in the BCK.ANN package would be available to users of the GUI. To a large extent this goal has been achieved, and indeed in some ways, surpassed. As development proceeded we recognised that there were certain features of ANNs which were only meaningful when presented in graphical form, such as Global Error evolution, connectivity patterns, Hinton diagrams of weight vectors and pseudo 3-dimensional maps of the functions implemented by groups of neurons. As such, we believe that in many ways, the BCK application with its GUI offers users more powerful techniques for ANN training and manipulation than the ANN package in the hands of a Java programmer. For this reason, we consider the GUI development to have been very successful.


Details

The BCK.GUI package contains 47 classes, most of which implement some graphical component such as dialog boxes, dragable icons, or simpler components such as text boxes which only accept numbers as input and so on.

The most sophisticated and important classes in the package are those which extend classes from the BCK.ANN package. Each class implementing a neural network is sub-classed in the BCK.GUI package. Additional methods and attributes have been added to these classes so that they can be represented graphically, and so that the functionality present in the superclasses can be accessed by the user through the GUI.

Despite the fact that many components in the GUI share much in common, it is very difficult to collect such shared attributes into coherent groups so that superclasses can be created and the inheritance mechanism used to promote code re-use. This is primarily because of the fact that each GUI component has very specific layout and functionality, and typically the methods in a GUI component class are handlers for events which can only arise in that particular class. Therefore, inheritance is simply not appropriate for such cases.

Each network subclass in the GUI consists of two main parts. The first is concerned with giving the network a graphical representation, in the form of an icon that can be dragged across the main application window. The second is an inner class which implements a complex dialog which allows the user to train and test the network. It was necessary to give each Network its own training dialog since the algorithms are quite different, and require different types of user input.

For more details, see the javadoc-generated documentation and source code.


Potential Improvements

The BCK as it now stands is the realisation of some ideas that we had concerning a development system for multi-network neural network systems. Not all of our original ideas made it into the BCK, for a variety of reasons, including time-constraints. Here is our 'wish-list' for future development:

The full implementation of Time Delay functionality. At the moment, the BCKNeuron class implements time delay features, but as of yet, we have not implemented an architecture with a learning algorithm which exploits this feature fully.
The addition of more ANN architectures to the BCK.ANN package.
The implementation of a training algorithm which operates across all networks in the brain.
